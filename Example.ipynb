{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bb8594d-befa-43eb-9d84-333269d98f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6862, 0.3138]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # print(outputs[\"last_hidden_state\"].shape)\n",
    "        # print(outputs[\"pooler_output\"].shape)\n",
    "        pooled_output = outputs[1]  # Take the pooled output (CLS token representation)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the model\n",
    "num_labels = 2  # For binary classification, adjust for multi-class\n",
    "model = BertForSequenceClassification(num_labels)\n",
    "\n",
    "# Example input text\n",
    "input_text = \"This is an example sentence.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "del inputs[\"token_type_ids\"]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(**inputs)\n",
    "\n",
    "print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6c32f05-07d2-4afe-96ba-dda0be67f259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101,  1996,  3185,  2001,  2200, 10634,  1998, 11771,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2023,  2143,  2001,  6659,  1012,  1045, 13842,  2048,  2847,\n",
      "          1997,  2026,  2166,  1012,   102,     0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), tensor([0, 0])]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sample data\n",
    "sample_data = [\n",
    "    {\"text\": \"I absolutely loved this movie! It was fantastic and well-acted.\", \"label\": 1},\n",
    "    {\"text\": \"This film was terrible. I wasted two hours of my life.\", \"label\": 0},\n",
    "    {\"text\": \"An excellent movie with a gripping story and brilliant performances.\", \"label\": 1},\n",
    "    {\"text\": \"Horrible movie. Poor storyline and bad acting.\", \"label\": 0},\n",
    "    {\"text\": \"A wonderful experience! I would watch it again.\", \"label\": 1},\n",
    "    {\"text\": \"Not worth the time. The plot was very predictable.\", \"label\": 0},\n",
    "    {\"text\": \"Amazing direction and acting. A must-watch!\", \"label\": 1},\n",
    "    {\"text\": \"The movie was very dull and boring.\", \"label\": 0},\n",
    "    {\"text\": \"Loved the cinematography and the plot twists.\", \"label\": 1},\n",
    "    {\"text\": \"The characters were not believable at all.\", \"label\": 0}\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Tokenize data\n",
    "inputs = tokenizer(df['text'].tolist(), return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(df['label'].tolist())\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Print sample from DataLoader\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dc23ada-c380-49a5-809a-e22235e9705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.7113\n",
      "Epoch 2/3, Loss: 0.6900\n",
      "Epoch 3/3, Loss: 0.6494\n",
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Fine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce86bfd7-b364-4bba-badd-1777f5c28937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5379, 0.4621]) torch.Size([2])\n",
      "Input Text: This movie was alright, not quite sure! It could be improved. But good characters!\n",
      "Predicted Label: Negative\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "input_text = \"This movie was alright, not quite sure! It could be improved. But good characters!\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "del inputs[\"token_type_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs[0]\n",
    "\n",
    "print(logits, logits.shape)\n",
    "predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "label_map = {0:\"Negative\", 1:\"Positive\"}\n",
    "predicted_label = label_map[predicted_class]\n",
    "\n",
    "print(f\"Input Text: {input_text}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae26660b-f0e2-4205-85fa-0b6729f95337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
